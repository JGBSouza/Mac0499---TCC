%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo é parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{O Kernel Linux}

Em um computador, o Sistema Operacional é a parte responsável por lidar com as interações entre o hardware e o software, permitindo, de maneira eficiente, a interação final máquina-usuário. Para que isso seja possível, o sistema operacional precisa ser dividido em diversas partes, dentre essas, o kernel, considerado o núcleo dos sistemas operacionais. Em geral, o kernel é um programa que opera a todo momento e é responsável pelo gerenciamento dos processos do sistema, alocando recursos para outros programas em atividade conforme a necessidade e prioridade de cada um.

Dentre os muitos sistemas de kernels existentes, o Kernel Linux é um projeto desenvolvido por Linus Torvalds em 1991 como alternativa ao Unix, pioneiro no mercado de sistemas operacionais. Hoje, o Kernel Linux é o maior projeto de software livre do mundo, utilizado por algumas das maiores empresas de tecnologia, software e computação no mercado, possuindo diversas distribuições e constituindo aproximadamente 57\% dos websites na internet cujos sistemas operacionais puderam ser identificados\footnote{Fonte: \href{https://w3techs.com/technologies/details/os-linux}{https://w3techs.com/technologies/details/os-linux}}.

\section{O Modelo de desenvolvimento do Kernel Linux}

Ainda que tenha sido lançado há mais três décadas, novas versões do Kernel Linux continuam sendo lançadas até hoje. Essas versões, chamadas “kernels estáveis”, são construídas, em parte, através da contribuição de diversos desenvolvedores ao redor do mundo, através da submissão de \textit{patches} de melhorias que são integrados à versão principal para o desenvolvimento de futuras versões. De acordo com \cite{feit2012perpetual}, esse desenvolvimento do Kernel Linux segue um modelo de perpetual development, no qual novas funcionalidades, correções e versões de produção são liberadas continuamente, havendo também a manutenção de versões mais antigas.

Segundo a \cite{linux_kernel_documentation}, esse processo divide-se em três etapas bem distintas, a janela de mesclagem, o período de estabilização e a manutenção contínua:

Durante a primeira etapa a maior parte das alterações serão integradas à nova versão do kernel. Essas mesclagens ocorrem a partir de \textit{patches} que foram previamente preparados, testados e coletados. Com base nessa nova versão, o primeiro kernel RC (Release Candidate) será lançado, encerrando a janela de mesclagem e iniciando a próxima etapa. 

Durante a segunda etapa, apenas \textit{patches} que sirvam para correção de \textit{bugs} deverão ser enviados e novas versões de RC serão lançadas periodicamente até que uma versão estável seja atingida. De forma objetiva, uma versão estável é atingida quando todas as regressões, erros conhecidos superados por versões anteriores, reintroduzidos durante a janela de mesclagem, são corrigidas. 

Contudo, dado o tempo limitado em que as etapas precisam ocorrer, eliminar todas as regressões das versões estáveis nem sempre é um desafio que pode ser atingido. Por conta desse fato, após a criação da versão estável, uma equipe de desenvolvedores é designada para a terceira etapa, a manutenção contínua, lançando novas atualizações ocasionais com correções para essa versão por um período de tempo enquanto a janela de mesclagem se reinicia para a nova versão.

\section{Software livre e contribuição para o Kernel Linux}

\subsection{Software livre x Software proprietário}

No mercado computacional, dois conceitos principais dominam o cenário no que se refere ao desenvolvimento como software proprietário ou software livre. Em geral, define-se como software proprietário, o software que é desenvolvido de maneira privada, com apenas a aplicação sendo acessível para usuários. Enquanto que, softwares livres são projetos cujo código fonte é de livre acesso para qualquer um que queira estudar.

Historicamente, o mercado computacional era dominado por grandes corporações, que detinham o monopólio do processo e conhecimento e recursos necessários para o desenvolvimento e do software como um todo, dificultando o ingresso de outros competidores no mercado. Nesse cenário, projetos de software livre surgem como um processo disruptivo, compartilhando o acesso a esse conhecimento, de modo a promover a colaboração e inovação na indústria \parencite{avatavului2023faircomparison}.

Comparativamente, softwares proprietários são geridos por empresas, que contratam equipes fixas de funcionários para trabalhar em período integral e de maneira exclusiva no projeto. Dessa maneira, a decisão quanto às novas implementações para o software são centradas, com o principal objetivo, em muitos casos, sendo o de ganho financeiro. Essa prática, contudo, muitas vezes implica em que o foco constante seja em implementações de novas ferramentas ao invés da melhoria do software como um todo, favorecendo com que esses softwares sejam mais propensos à erros e falhas ocasionais. Por outro lado, a existência de responsáveis legais pelo projeto fazem com que esses softwares contem em grande parte com a existência de um suporte especializado, característica valorizada no mercado corporativo.

De maneira contraditória, softwares livres são desenvolvidos de maneira colaborativa, contando com a contribuição voluntária de grandes quantidades de desenvolvedores ao redor do mundo inteiro. Por conta disso, as demandas surgem de forma espontânea, muitas vezes da necessidade do próprio usuário que depende do software para usos pessoais. Essa grande quantidade de contribuidores, aliada à dependência mútua destes com o software, garante atualizações frequentes de segurança e qualidade. Como consequência, esses sistemas tendem a ser menos propensos a erros, mas não contam com um suporte dedicado na maioria dos casos.

Hoje, softwares livres e proprietários continuam coexistindo no mercado, sendo constantemente comparados quanto à sua efetividade observada em projetos reais. Porém, a inegável vantagem do conhecimento colaborativo e do grande volume de contribuição que softwares livres conseguem apresentar, fazem com que hoje ele esteja em grande crescente no mercado computacional, sendo o método de desenvolvimento de diversos softwares relevantes mundialmente, como no caso do Kernel Linux.

\subsection{Contribuição em Software Livres}

Em projetos de software livre, para que a gestão das contribuições seja possível, os sistemas geralmente contam com uma equipe mantenedores, grupo interno de desenvolvedores que possuem responsabilidade geral pelo código principal. Desse modo, para que sejam integradas, as contribuições enviadas precisam passar por revisões por parte dos mantenedores para garantir que atendam aos requisitos técnicos definidos para o projeto. De acordo com \cite{tan2020scaling}, os mantenedores devem avaliar principalmente se um \textit{patch} é necessário, se uma implementação apresenta falhas ou se existem eventuais melhorias na forma como a solução foi feita.

Em alguns casos, principalmente devido ao crescimento dos projetos, torna-se necessário também uma divisão em componentes do sistema principal, de modo que cada parte possui seus mantenedores dedicados. Dessa maneira, para que a contribuição seja feita de maneira correta, os \textit{patches} precisam ser enviados diretamente para o responsável do subsistema que será alterado.

\subsection{Contribuindo para o Kernel Linux} 

No caso do Kernel Linux, essa divisão lógica é feita com base em conjuntos de subsistemas, como, por exemplo, os sistema de rede, gerenciamento de memória, dispositivos de vídeo, etc. Dentro desses subsistemas, cada mantenedor responsável administra uma árvore de fontes do kernel, gerindo os \textit{patches} enviados ao seu subsistema. Assim, quando a janela de mesclagem é aberta, às contribuições previamente aprovadas pelos mantenedores serão enviadas diretamente ao Linus e, caso aceitas, integradas ao código principal para gerar a nova versão. (\cite{linux_kernel_documentation})

Ainda segundo a documentação oficial (\cite{linux_kernel_documentation}), eventualmente, esses subsistemas podem ser identificados de modo que um subsistema principal seja constituído por subsistemas menores, como, por exemplo, o subsistema de rede, que agrega também as árvores dedicadas a drivers de dispositivos de rede, redes sem fio. Desse modo, além dos \textit{patches} recebidos diretamente por contribuidores, os mantenedores podem receber também \textit{patches} já aprovados por outros mantenedores, formando uma cadeia de confiança até que os \textit{patches} cheguem a serem integrados.

Assim como o kernel, os \textit{patches} que são incorporados durante a janela de mesclagem precisam ser previamente preparados e, durante esse preparo, passam por algumas etapas. Esse processo, ainda que informal, serve para garantir que cada \textit{patch} possa ser revisado e tenha sua qualidade garantida antes que a alteração seja incorporada ao kernel principal (\cite{linux_kernel_documentation}).

Os principais estágios que um \textit{patch} deve passar, são: 

\begin{enumerate}
    \item \textbf{Design}: Nesta etapa, serão levantados os requisitos do \textit{patch} e a forma com que serão atingidos, ou seja, a identificação dos seus objetivos e as necessidades técnicas que devem constar nessa implementação.
    \item \textbf{Revisão antecipada}: Publicação dos \textit{patches} na lista de discussão relevante para que desenvolvedores possam responder com comentários e ajudar a revelar quaisquer problemas iniciais.
    \item \textbf{Revisão mais ampla}: Antes que o \textit{patch} seja considerado para inclusão na versão principal, ele deve ser aceito por um mantenedor de subsistema, que o incluirá nas árvores \textit{-next}. Com essa etapa, revisões mais elaboradas e possíveis problemas de integração com outras implementações poderão ser verificados.
    \item \textbf{Mesclagem e manutenção de longo prazo}: Ainda que o \textit{patch} possa ser mesclado e chegar efetivamente à versão estável do kernel, futuros problemas podem vir a aparecer durante essas fases, dessa forma, o desenvolvedor original deve continuar a assumir a responsabilidade da manutenção do código no futuro.
\end{enumerate}
\subsection{O problema da contribuição em desenvolvimento de software}

O sistema de controle de versão é indispensável no desenvolvimento de software contemporâneo. Antes do advento dos sistemas de controle de versão, os programadores dependiam de métodos manuais para gerenciar suas modificações de código. Eles costumavam fazer backups regulares de seus arquivos de código ou adotar convenções de nomenclatura para distinguir entre as várias versões. Esse processo era bastante inconsistente e difícil de gerenciar, especialmente quando alguns desenvolvedores estavam trabalhando no mesmo projeto. \parencite{devineni2020vcs}

Gerenciar as versões de um software se torna um problema ainda maior dependendo do tamanho total do software, do número de contribuidores e da quantidade de contribuições sendo realizadas nele de maneira simultânea. No kernel, por exemplo, a versão 6.13, lançada em 19/01/2025, contou com mais de 206 contribuições por dia por parte de 2085 colaboradores, resultando em um código fonte final com mais de 39 milhões de linhas \cite{kernelhistory}. Segundo a tendência, esses números devem seguir aumentando de forma constante conforme novas versões forem sendo desenvolvidas.

Buscando superar parte dessas dificuldades e melhorar o processo colaborativo de desenvolvimento de software, foram desenvolvidos os sistemas de controle de versão. Segundo Devineni (2020),

Os primeiros Version Control Systems (VCS), permitiam que os desenvolvedores mantivessem um histórico das alterações realizadas nos arquivos, o que facilitava a reversão de mudanças e oferecia visibilidade sobre a evolução do código. No entanto, o potencial colaborativo ainda era limitado, exigindo muitos acordos e gestões manuais por parte dos colaboradores.

Como segunda opção, surgem os \textit{Concurrent Versions System} \textit{(CVS)}, baseados em um modelo de repositório central. Nele, os desenvolvedores podiam obter os arquivos, aplicar suas modificações e submetê-las novamente ao repositório. Esse modelo contribuiu para maior agilidade em equipes de desenvolvimento, ao permitir que várias pessoas trabalhassem simultaneamente na mesma base de código. Ainda assim, em projetos de grande porte ou com equipes distribuídas geograficamente, os sistemas centralizados apresentavam limitações no gerenciamento eficiente do trabalho.

Por fim, surgem os modelos mais utilizados atualmente, os \textit{Distributed Concurrent Versions System} - \textit{DVCS}, como o Git. Esses sistemas, ao contrário da versão anterior, distribuía as cópias do código central entre os desenvolvedores, permitindo um método mais flexível de colaboração. Como cada colaborador poderia ter uma versão local do código, as mudanças realizadas por ele ao código principal poderiam ser administradas localmente antes de serem integradas, permitindo trabalhos offline e que alterações fossem submetidas em lotes ao invés de individualmente.

Contudo, de acordo com \cite{kernelrecipes}, ainda que softwares como \textit{github}\footnote{https://github.com}, \textit{gerrit}\footnote{https://www.gerritcodereview.com} ou outros DVCS possam ser úteis para gerir o fluxo de submissões de softwares menores, eles ainda apresentam muitos problemas para escalar para softwares maiores. Dentre os principais motivos, são citados, por exemplo, a maneira como o fluxo para revisão desses softwares é mais demorado e diminui a produtividade dos mantenedores, a dificuldade de gerenciar e categorizar os inúmeros problemas e submissões com os recursos oferecidos, a maneira como as discussões e comentários dentro da comunidade são pouco acessíveis à outros contribuidores, dificultando a propagação de informação e gerando retrabalho, a dificuldade para que desenvolvedores possam se conectar à listas de discussões e serem notificados sempre que uma novidade relevante ocorra, entre outros. Parte desses problemas da comunidade, porém, ainda segundo \cite{kernelrecipes}, são solucionados ao se substituir os softwares de DVCS por servidores de email, como é feito para a contribuição do kernel.

Essa substituição, entre tanto, também apresenta suas dificuldades, uma vez que, sendo um sistema com perspectiva muito mais abrangente, servidores de email não apresentam funcionalidades e melhorias para esse fluxo. Entre os diversos problemas enfrentados pelo usuário, destacam-se principalmente o grande \textit{overhead} inicial para novos contribuidores, a má rastreabilidade do históricos de submissões e revisões, a escalabilidade limitada, sobrecarregando a lista de email de alguns mantenedores, problemas de corrupção de arquivos, e a dificuldade de se capturar métricas. Em resposta, hoje, muitos softwares vêm sendo desenvolvidos, buscando oferecendo automatizações para esse fluxo na busca de suprir algumas de suas lacunas, como é o caso do \textit{b4}\footnote{\url{https://b4.docs.kernel.org/en/latest/}}, \textit{Patchwork}\footnote{\url{https://patchwork.kernel.org/}}, \textit{lore.kernel.org}\footnote{\url{https://lore.kernel.org/}}, \textit{kworkflow}\footnote{\url{https://github.com/kworkflow/kworkflow}}, entre outras soluções encontradas na comunidade. 
